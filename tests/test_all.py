#!/usr/bin/env python3
"""
Master test script for sensortower-mcp - runs all testing suites.
Use this to get a comprehensive production readiness assessment.
"""

import asyncio
import subprocess
import sys
import os
import time
from pathlib import Path
from typing import Dict, List, Tuple

# Add parent directory to path to import main module
sys.path.insert(0, str(Path(__file__).parent.parent))

class MasterTester:
    def __init__(self):
        self.results = {}
        self.start_time = time.time()
        self.tests_dir = Path(__file__).parent
    
    def print_header(self, title: str):
        print(f"\n{'='*60}")
        print(f"üß™ {title}")
        print(f"{'='*60}")
    
    def print_section(self, title: str):
        print(f"\n{'‚îÄ'*40}")
        print(f"üìã {title}")
        print(f"{'‚îÄ'*40}")
    
    def run_script(self, script_name: str, description: str) -> Tuple[bool, str]:
        """Run a test script and capture results"""
        print(f"\nüöÄ Running {description}...")
        
        script_path = self.tests_dir / script_name
        if not script_path.exists():
            return False, f"Script {script_name} not found"
        
        try:
            result = subprocess.run(
                [sys.executable, str(script_path)],
                capture_output=True,
                text=True,
                timeout=300,  # 5 minutes max per script
                cwd=self.tests_dir.parent  # Run from project root
            )
            
            success = result.returncode == 0
            output = result.stdout if success else result.stderr
            
            # Show summary
            if success:
                print("‚úÖ PASSED")
            else:
                print("‚ùå FAILED")
                print(f"Error output: {result.stderr[:200]}...")
            
            return success, output
            
        except subprocess.TimeoutExpired:
            return False, "Test script timed out (>5 minutes)"
        except Exception as e:
            return False, f"Failed to run script: {e}"
    
    def check_prerequisites(self) -> Dict[str, bool]:
        """Check if all prerequisites are available"""
        self.print_section("Prerequisites Check")
        
        prereqs = {
            "Python": True,  # We're running Python
            "Docker": False,
            "API Token": False,
            "Dependencies": False
        }
        
        # Check Docker
        try:
            result = subprocess.run(["docker", "--version"], capture_output=True, timeout=10)
            prereqs["Docker"] = result.returncode == 0
            if prereqs["Docker"]:
                print("‚úÖ Docker available")
            else:
                print("‚ùå Docker not available")
        except:
            print("‚ùå Docker not found")
        
        # Check API token
        token = os.getenv("SENSOR_TOWER_API_TOKEN")
        prereqs["API Token"] = bool(token)
        if prereqs["API Token"]:
            print("‚úÖ API token set")
        else:
            print("‚ö†Ô∏è  API token not set (some tests will be limited)")
        
        # Check dependencies
        try:
            import httpx
            import aiohttp
            prereqs["Dependencies"] = True
            print("‚úÖ Test dependencies available")
        except ImportError:
            print("‚ùå Missing dependencies. Run: pip install httpx aiohttp")
        
        return prereqs
    
    def run_all_tests(self, prereqs: Dict[str, bool]):
        """Run all test suites"""
        self.print_section("Running Test Suites")
        
        # Test suite configurations
        test_suites = [
            {
                "script": "test_manual.py",
                "name": "Manual Testing",
                "description": "Quick manual validation",
                "required_prereqs": ["Dependencies"],
                "critical": True
            },
            {
                "script": "test_deployment.py", 
                "name": "Deployment Testing",
                "description": "Comprehensive PyPI and Docker testing",
                "required_prereqs": ["Dependencies", "Docker"],
                "critical": True
            },
            {
                "script": "test_security.py",
                "name": "Security Testing", 
                "description": "Security best practices validation",
                "required_prereqs": ["Dependencies"],
                "critical": True
            },
            {
                "script": "test_load.py",
                "name": "Load Testing",
                "description": "Performance under concurrent load",
                "required_prereqs": ["Dependencies", "Docker"],
                "critical": False
            }
        ]
        
        for suite in test_suites:
            # Check if prerequisites are met
            missing_prereqs = [p for p in suite["required_prereqs"] if not prereqs.get(p, False)]
            
            if missing_prereqs:
                print(f"\n‚è≠Ô∏è  Skipping {suite['name']}: Missing {', '.join(missing_prereqs)}")
                self.results[suite["name"]] = {
                    "status": "SKIPPED",
                    "reason": f"Missing prerequisites: {', '.join(missing_prereqs)}",
                    "critical": suite["critical"]
                }
                continue
            
            # Run the test
            success, output = self.run_script(suite["script"], suite["description"])
            
            self.results[suite["name"]] = {
                "status": "PASSED" if success else "FAILED",
                "output": output,
                "critical": suite["critical"]
            }
    
    def generate_comprehensive_report(self):
        """Generate final comprehensive report"""
        self.print_header("COMPREHENSIVE PRODUCTION READINESS REPORT")
        
        total_time = time.time() - self.start_time
        
        # Summary statistics
        total_tests = len(self.results)
        passed = sum(1 for r in self.results.values() if r["status"] == "PASSED")
        failed = sum(1 for r in self.results.values() if r["status"] == "FAILED")
        skipped = sum(1 for r in self.results.values() if r["status"] == "SKIPPED")
        critical_failed = sum(1 for r in self.results.values() if r["status"] == "FAILED" and r["critical"])
        
        print(f"\nüìä TEST EXECUTION SUMMARY")
        print(f"Total execution time: {total_time:.1f} seconds")
        print(f"Test suites run: {total_tests}")
        print(f"‚úÖ Passed: {passed}")
        print(f"‚ùå Failed: {failed}")
        print(f"‚è≠Ô∏è  Skipped: {skipped}")
        print(f"üö® Critical failures: {critical_failed}")
        
        # Detailed results
        print(f"\nüìã DETAILED RESULTS")
        for test_name, result in self.results.items():
            status_emoji = {
                "PASSED": "‚úÖ",
                "FAILED": "‚ùå", 
                "SKIPPED": "‚è≠Ô∏è"
            }[result["status"]]
            
            critical_marker = " üö®" if result.get("critical", False) and result["status"] == "FAILED" else ""
            print(f"{status_emoji} {test_name}{critical_marker}")
            
            if result["status"] == "SKIPPED":
                print(f"   Reason: {result['reason']}")
            elif result["status"] == "FAILED":
                print(f"   Check the output above for details")
        
        # Production readiness assessment
        print(f"\nüéØ PRODUCTION READINESS ASSESSMENT")
        
        if critical_failed > 0:
            print("üî¥ NOT READY FOR PRODUCTION")
            print("‚ùå Critical test failures must be resolved before deployment")
            print("\nRequired actions:")
            for test_name, result in self.results.items():
                if result["status"] == "FAILED" and result["critical"]:
                    print(f"  - Fix issues in {test_name}")
        elif failed > 0:
            print("üü° READY WITH CAUTIONS")
            print("‚ö†Ô∏è  Some non-critical tests failed - review recommended")
            print("\nRecommended actions:")
            for test_name, result in self.results.items():
                if result["status"] == "FAILED":
                    print(f"  - Review issues in {test_name}")
        elif skipped > 0:
            print("üü° PARTIALLY VALIDATED")
            print("‚ö†Ô∏è  Some tests were skipped due to missing prerequisites")
            print("\nTo complete validation:")
            for test_name, result in self.results.items():
                if result["status"] == "SKIPPED":
                    print(f"  - {test_name}: {result['reason']}")
        else:
            print("üü¢ READY FOR PRODUCTION")
            print("‚úÖ All tests passed successfully!")
        
        # Deployment checklist
        print(f"\nüìù DEPLOYMENT CHECKLIST")
        checklist_items = [
            ("Set SENSOR_TOWER_API_TOKEN", bool(os.getenv("SENSOR_TOWER_API_TOKEN"))),
            ("Docker image available", "Docker" in [t for t in self.results.keys() if self.results[t]["status"] == "PASSED"]),
            ("PyPI package tested", "Manual Testing" in [t for t in self.results.keys() if self.results[t]["status"] == "PASSED"]),
            ("Security validated", "Security Testing" in [t for t in self.results.keys() if self.results[t]["status"] == "PASSED"]),
            ("Performance tested", "Load Testing" in [t for t in self.results.keys() if self.results[t]["status"] == "PASSED"]),
            ("Health checks working", True),  # Assume working if other tests pass
            ("Resource limits configured", True),  # Checked in deployment tests
            ("Monitoring planned", False),  # User needs to implement
            ("Backup strategy planned", False),  # User needs to implement
            ("Rollback procedure tested", False)  # User needs to implement
        ]
        
        for item, completed in checklist_items:
            status = "‚úÖ" if completed else "‚ùå"
            print(f"{status} {item}")
        
        # Final recommendations
        print(f"\nüí° FINAL RECOMMENDATIONS")
        
        recommendations = [
            "Test in staging environment before production deployment",
            "Set up monitoring and alerting for health endpoints", 
            "Configure log aggregation for troubleshooting",
            "Implement secrets management (avoid environment variables in production)",
            "Set up automated security scanning in CI/CD pipeline",
            "Plan for scaling based on load testing results",
            "Document rollback procedures",
            "Schedule regular dependency updates",
            "Consider implementing rate limiting for production use",
            "Set up automated backups if stateful data exists"
        ]
        
        for i, rec in enumerate(recommendations, 1):
            print(f"{i:2d}. {rec}")
        
        # Return exit code
        return 1 if critical_failed > 0 else 0

async def main():
    """Main execution function"""
    print("üöÄ Sensor Tower MCP - Comprehensive Testing Suite")
    print("This will run all available tests and generate a production readiness report")
    
    tester = MasterTester()
    
    # Check prerequisites
    prereqs = tester.check_prerequisites()
    
    # Ask user if they want to continue with missing prerequisites
    missing = [k for k, v in prereqs.items() if not v and k != "API Token"]
    if missing:
        print(f"\n‚ö†Ô∏è  Missing prerequisites: {', '.join(missing)}")
        response = input("Continue anyway? (y/N): ")
        if response.lower() != 'y':
            print("Exiting. Please install missing prerequisites and try again.")
            return 1
    
    # Run all tests
    tester.run_all_tests(prereqs)
    
    # Generate report and exit with appropriate code
    exit_code = tester.generate_comprehensive_report()
    
    return exit_code

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code) 